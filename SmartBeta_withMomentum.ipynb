{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEojS2H9cCjy"
      },
      "source": [
        "# Algothon 2023 x Fyde Treasury Protocol\n",
        "*Momentum Capture for Smart Beta ETF*\n",
        "\n",
        "## Instructions\n",
        "Fyde_Algothon_Instructions.pdf contain challenge objectives, grading, and submission instructions.\n",
        "README.md contains technical and installation instructions.\n",
        "\n",
        "We have provided you with a basic script that contains the following functionalities:\n",
        "1.   Load the provided synthetic data\n",
        "2.   Apply an initial selection filter to choose assets\n",
        "3.   Create a market-weighted index benchmark\n",
        "4.   Optimize the manager's ETF portfolio\n",
        "5.   Calculate a rebalancer for the portfolio\n",
        "6.   Evaluate performance using variety of metrics\n",
        "\n",
        "The main areas for improvement are sections 2, 4, and 5.\n",
        "\n",
        "## Objective\n",
        "Your job will be to build a mechanism to capture downward momentum for the portfolio. You can update any of the provided code or create new functions to accomplish your task.\n",
        "\n",
        "\n",
        "## Note:\n",
        "We encourage out-of-the-box thinking and the generation of new ideas. While the foundation is important, the sky's the limit when it comes to innovation. \n",
        "\n",
        "Good luck!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SZ-wLkIk8Qi"
      },
      "source": [
        "**Install Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa-5cppBcCj1"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbMvqF6zcCj3"
      },
      "source": [
        "**Load Packages**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "asridlDkcCj3",
        "outputId": "e6864eb0-648a-4474-fd01-f90dcce3c056"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import helper\n",
        "import project_helper\n",
        "import project_tests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3v52lSEccCj4"
      },
      "source": [
        "## Section 1: Load Market Data\n",
        "\n",
        "---\n",
        "\n",
        "The provided CSV file contains the same underlying distribution as the comparison data that will be used for grading purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gaIUxCWMcCj4"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('Fyde_data.csv',parse_dates=['date'], dayfirst=True)\n",
        "df['date'] = df['date'].dt.strftime('%Y-%m-%d-%H-%M-%S')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUQR2sVcmvnT"
      },
      "source": [
        "## Section 2: Apply selection filter\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "For this initial example we will be selecting the top 50% of volume compared to all assets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pex3ayw-muvZ"
      },
      "outputs": [],
      "source": [
        "percent_top_dollar = 0.5\n",
        "high_volume_symbols = project_helper.large_dollar_volume_tokens(df, 'adj_close', 'adj_volume', percent_top_dollar)\n",
        "df = df[df['ticker'].isin(high_volume_symbols)]\n",
        "\n",
        "df = df.set_index(['date', 'ticker']).sort_index()\n",
        "\n",
        "close = df.reset_index().pivot(index='date', columns='ticker', values='adj_close')\n",
        "volume = df.reset_index().pivot(index='date', columns='ticker', values='adj_volume')\n",
        "market_cap = df.reset_index().pivot(index='date', columns='ticker', values='adj_market_cap')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBxlzac4cCj5"
      },
      "source": [
        "**View Data:** To see what one of these 2-d matrices looks like, let's take a look at the closing prices matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFiLAvuicCj5"
      },
      "outputs": [],
      "source": [
        "project_helper.print_dataframe(close)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6Yv_v13cCj5"
      },
      "source": [
        "## Section 3: Market Weight Index\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Here we will create the market weighted index that we will eventually use to compare against your ETF portfolio to see how well it performs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APIzy7V9cCj5"
      },
      "outputs": [],
      "source": [
        "def generate_market_cap_weights(close, market_cap):\n",
        "    \"\"\"\n",
        "    Generate market capitalization weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    close : DataFrame\n",
        "        Close price for each ticker and date\n",
        "    market_cap : str\n",
        "        market_cap for each ticker and date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    market_cap_weights : DataFrame\n",
        "        The market cap weights for each ticker and date\n",
        "    \"\"\"\n",
        "    assert close.index.equals(market_cap.index)\n",
        "    assert close.columns.equals(market_cap.columns)\n",
        "\n",
        "\n",
        "    weights = market_cap\n",
        "    weights = weights.apply(lambda row: row / np.sum(row), axis = 1)\n",
        "    \n",
        "    return weights\n",
        "\n",
        "project_tests.test_generate_market_cap_weights(generate_market_cap_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydEnYQ86cCj6"
      },
      "source": [
        "**View Data:**\n",
        "Let's generate the index weights using `generate_market_cap_weights` and view them using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Sofu057cCj6"
      },
      "outputs": [],
      "source": [
        "index_weights = generate_market_cap_weights(close, market_cap)\n",
        "project_helper.plot_weights(index_weights, 'Index Weights')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBTXqSpfcCj8"
      },
      "source": [
        "**Returns:**\n",
        "Implement `generate_returns` to generate returns data for all the assets and dates from price data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xWHEPnucCj8"
      },
      "outputs": [],
      "source": [
        "def generate_returns(prices):\n",
        "    \"\"\"\n",
        "    Generate returns for ticker and date.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    prices : DataFrame\n",
        "        Price for each ticker and date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns : Dataframe\n",
        "        The returns for each ticker and date\n",
        "    \"\"\"\n",
        "    \n",
        "    log_returns = np.log(prices / prices.shift(1))\n",
        "\n",
        "    return log_returns\n",
        "\n",
        "project_tests.test_generate_returns(generate_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pc5MuAzcCj8"
      },
      "source": [
        "**View Data:**\n",
        "Let's generate the closing returns using `generate_returns` and view them using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_0Dyu1CcCj9"
      },
      "outputs": [],
      "source": [
        "returns = generate_returns(close)\n",
        "project_helper.plot_returns(returns, 'Close Returns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZ46CKQ4cCj9"
      },
      "source": [
        "**Weighted Returns:** With the returns of assets computed, we can use it to compute the returns for an index or ETF. Implement `generate_weighted_returns` to create weighted returns using the returns and weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kZyrieeAcCj9"
      },
      "outputs": [],
      "source": [
        "def generate_weighted_returns(returns, weights):\n",
        "    \"\"\"\n",
        "    Generate weighted returns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : DataFrame\n",
        "        Returns for each ticker and date\n",
        "    weights : DataFrame\n",
        "        Weights for each ticker and date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    weighted_returns : DataFrame\n",
        "        Weighted returns for each ticker and date\n",
        "    \"\"\"\n",
        "    assert returns.index.equals(weights.index)\n",
        "    assert returns.columns.equals(weights.columns)\n",
        "\n",
        "    return returns * weights\n",
        "\n",
        "project_tests.test_generate_weighted_returns(generate_weighted_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WgNuwQlcCj9"
      },
      "source": [
        "**View Data:** Let's generate the ETF and index returns using `generate_weighted_returns` and view them using a heatmap."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FfKX74xcCj-"
      },
      "outputs": [],
      "source": [
        "index_weighted_returns = generate_weighted_returns(returns, index_weights)\n",
        "project_helper.plot_returns(index_weighted_returns, 'Index Returns')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GXFq1IRcCkA"
      },
      "source": [
        "## Section 4: Portfolio Optimization\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Note*: We  want to maintain a hard rule that the **weights sum to one and be long only, so no negative weights.**\n",
        "\n",
        "Otherwise you are free to change any of these calculations, objectives or methodology.\n",
        "\n",
        "\n",
        "\n",
        "Now, let's create our own ETF portfolio that we will compare against the market cap weighted index. Initially,  we're minimizing the distance between the weights of our portfolio and the weights of the index.\n",
        "\n",
        "$Minimize \\left [ \\sigma^2_p + \\lambda \\sqrt{\\sum_{1}^{m}(weight_i - indexWeight_i)^2} \\right  ]$ where $m$ is the number of stocks in the portfolio, and $\\lambda$ is a scaling factor that you can choose.\n",
        "\n",
        "Why are we doing this? One way that investors evaluate a fund is by how well it tracks its index. The fund is still expected to deviate from the index within a certain range in order to improve fund performance.  \n",
        "\n",
        "\n",
        "##### Covariance\n",
        "Implement `get_covariance_returns` to calculate the covariance of the `returns`. We'll use this to calculate the portfolio variance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiNhYXiccCkK"
      },
      "outputs": [],
      "source": [
        "def get_covariance_returns(returns):\n",
        "    \"\"\"\n",
        "    Calculate covariance matrices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : DataFrame\n",
        "        Returns for each ticker and date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    returns_covariance  : 2 dimensional Ndarray\n",
        "        The covariance of the returns\n",
        "    \"\"\"\n",
        "    return np.cov( returns.fillna(0).T )\n",
        "\n",
        "\n",
        "project_tests.test_get_covariance_returns(get_covariance_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jw4AtuBcCkL"
      },
      "source": [
        "**View Data:**\n",
        "Let's look at the covariance generated from `get_covariance_returns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoaP1DkBcCkL"
      },
      "outputs": [],
      "source": [
        "covariance_returns = get_covariance_returns(returns)\n",
        "covariance_returns = pd.DataFrame(covariance_returns, returns.columns, returns.columns)\n",
        "\n",
        "covariance_returns_correlation = np.linalg.inv(np.diag(np.sqrt(np.diag(covariance_returns))))\n",
        "covariance_returns_correlation = pd.DataFrame(\n",
        "    covariance_returns_correlation.dot(covariance_returns).dot(covariance_returns_correlation),\n",
        "    covariance_returns.index,\n",
        "    covariance_returns.columns)\n",
        "\n",
        "project_helper.plot_covariance_returns_correlation(\n",
        "    covariance_returns_correlation,\n",
        "    'Covariance Returns Correlation Matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKC-qJvhcCkL"
      },
      "source": [
        "**Optimization:**\n",
        "cvxpy has the constructor `Problem(objective, constraints)`, which returns a `Problem` object.\n",
        "\n",
        "The `Problem` object has a function solve(), which returns the minimum of the solution.  In this case, this is the minimum variance of the portfolio.\n",
        "\n",
        "It also updates the vector $\\mathbf{x}$.\n",
        "\n",
        "We can check out the values of $x_A$ and $x_B$ that gave the minimum portfolio variance by using `x.value`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5b6R7CbcCkL"
      },
      "outputs": [],
      "source": [
        "import cvxpy as cvx\n",
        "\n",
        "def get_optimal_weights(covariance_returns, index_weights, scale=2.0):\n",
        "    \"\"\"\n",
        "    Find the optimal weights.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    covariance_returns : 2 dimensional Ndarray\n",
        "        The covariance of the returns\n",
        "    index_weights : Pandas Series\n",
        "        Index weights for all tickers at a period in time\n",
        "    scale : int\n",
        "        The penalty factor for weights the deviate from the index\n",
        "    Returns\n",
        "    -------\n",
        "    x : 1 dimensional Ndarray\n",
        "        The solution for x\n",
        "    \"\"\"\n",
        "    assert len(covariance_returns.shape) == 2\n",
        "    assert len(index_weights.shape) == 1\n",
        "    assert covariance_returns.shape[0] == covariance_returns.shape[1]  == index_weights.shape[0]\n",
        "\n",
        "    m = len(covariance_returns)\n",
        "    x = cvx.Variable(m)\n",
        "    portfolio_variance = cvx.quad_form(x, covariance_returns)\n",
        "    distance_to_index = cvx.norm(x - index_weights)\n",
        "    objectives = cvx.Minimize(portfolio_variance + scale * distance_to_index)\n",
        "    constraints = [x >= 0, sum(x) == 1]\n",
        "    result = cvx.Problem(objectives, constraints).solve()\n",
        "\n",
        "    return x.value\n",
        "\n",
        "project_tests.test_get_optimal_weights(get_optimal_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zBIijMmcCkM"
      },
      "source": [
        "**Optimized Portfolio**\n",
        "Using the `get_optimal_weights` function, let's generate the optimal ETF weights without rebalanceing. We can do this by feeding in the covariance of the entire history of data. We also need to feed in a set of index weights. We'll go with the average weights of the index over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GoeRwjHWcCkM"
      },
      "outputs": [],
      "source": [
        "raw_optimal_single_rebalance_etf_weights = get_optimal_weights(covariance_returns.values, index_weights.iloc[-1])\n",
        "optimal_single_rebalance_etf_weights = pd.DataFrame(\n",
        "    np.tile(raw_optimal_single_rebalance_etf_weights, (len(returns.index), 1)),\n",
        "    returns.index,\n",
        "    returns.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qis1FucycCkN"
      },
      "source": [
        "## Section 5: Rebalance Portfolio Over Time\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "The single optimized ETF portfolio used the same weights for the entire history. This might not be the optimal weights for the entire period. Let's rebalance the portfolio over the same period instead of using the same weights. Implement `rebalance_portfolio` to rebalance a portfolio.\n",
        "\n",
        "Reblance the portfolio every n number of days, which is given as `shift_size`. When rebalancing, you should look back a certain number of days of data in the past, denoted as `chunk_size`. Using this data, compute the optimimal weights using `get_optimal_weights` and `get_covariance_returns`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBIYYdDLcCkN"
      },
      "outputs": [],
      "source": [
        "def rebalance_portfolio(returns, index_weights, shift_size, chunk_size):\n",
        "    \"\"\"\n",
        "    Get weights for each rebalancing of the portfolio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : DataFrame\n",
        "        Returns for each ticker and date\n",
        "    index_weights : DataFrame\n",
        "        Index weight for each ticker and date\n",
        "    shift_size : int\n",
        "        The number of days between each rebalance\n",
        "    chunk_size : int\n",
        "        The number of days to look in the past for rebalancing\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    all_rebalance_weights  : list of Ndarrays\n",
        "        The ETF weights for each point they are rebalanced\n",
        "    \"\"\"\n",
        "    assert returns.index.equals(index_weights.index)\n",
        "    assert returns.columns.equals(index_weights.columns)\n",
        "    assert shift_size > 0\n",
        "    assert chunk_size >= 0\n",
        "\n",
        "    weights = []\n",
        "    for i in range(chunk_size, len(returns), shift_size):\n",
        "        covariance_returns = get_covariance_returns(returns[i-chunk_size : i])\n",
        "        weights.append(get_optimal_weights(covariance_returns, index_weights.iloc[i-1]) )\n",
        "    return weights\n",
        "\n",
        "project_tests.test_rebalance_portfolio(rebalance_portfolio)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsGHeSV1cCkO"
      },
      "source": [
        "Run the following cell to rebalance the portfolio using `rebalance_portfolio`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZtuuS_0cCkO"
      },
      "outputs": [],
      "source": [
        "chunk_size = 250\n",
        "shift_size = 5\n",
        "all_rebalance_weights = rebalance_portfolio(returns, index_weights, shift_size, chunk_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Portfolio Turnover:**\n",
        "With the portfolio rebalanced, we need to use a metric to measure the cost of rebalancing the portfolio. Implement `get_portfolio_turnover` to calculate the annual portfolio turnover.\n",
        "\n",
        "$ AnnualizedTurnover =\\frac{SumTotalTurnover}{NumberOfRebalanceEvents} * NumberofRebalanceEventsPerYear $\n",
        "\n",
        "$ SumTotalTurnover =\\sum_{t,n}{\\left | x_{t,n} - x_{t+1,n} \\right |} $ Where $ x_{t,n} $ are the weights at time $ t $ for equity $ n $.\n",
        "\n",
        "$ SumTotalTurnover $ is just a different way of writing $ \\sum \\left | x_{t_1,n} - x_{t_2,n} \\right | $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_portfolio_turnover(all_rebalance_weights, shift_size, rebalance_count, n_trading_days_in_year=252):\n",
        "    \"\"\"\n",
        "    Calculage portfolio turnover.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    all_rebalance_weights : list of Ndarrays\n",
        "        The ETF weights for each point they are rebalanced\n",
        "    shift_size : int\n",
        "        The number of days between each rebalance\n",
        "    rebalance_count : int\n",
        "        Number of times the portfolio was rebalanced\n",
        "    n_trading_days_in_year: int\n",
        "        Number of trading days in a year\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    portfolio_turnover  : float\n",
        "        The portfolio turnover\n",
        "    \"\"\"\n",
        "    assert shift_size > 0\n",
        "    assert rebalance_count > 0\n",
        "\n",
        "    sumTurnOver = 0\n",
        "    for i in range( len(all_rebalance_weights)-1 ):\n",
        "        sumTurnOver += np.sum( np.abs( all_rebalance_weights[i] - all_rebalance_weights[i+1] ) )\n",
        "    year_count = n_trading_days_in_year / shift_size\n",
        "    return sumTurnOver / rebalance_count * year_count\n",
        "\n",
        "project_tests.test_get_portfolio_turnover(get_portfolio_turnover)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the following cell to get the portfolio turnover from  `get_portfolio turnover`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(get_portfolio_turnover(all_rebalance_weights, shift_size, len(all_rebalance_weights) - 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96QqHjl_cCkN"
      },
      "source": [
        "## Section 6: Results Comparison\n",
        "With our ETF weights built, let's compare it to the index. Run the next cell to calculate the ETF returns and compare it to the index returns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Cumulative Returns:**\n",
        "To compare performance between the ETF and Index, we're going to calculate the tracking error. Before we do that, we first need to calculate the index and ETF comulative returns. Implement `calculate_cumulative_returns` to calculate the cumulative returns over time given the returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_cumulative_returns(returns):\n",
        "    \"\"\"\n",
        "    Calculate cumulative returns.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns : DataFrame\n",
        "        Returns for each ticker and date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    cumulative_returns : Pandas Series\n",
        "        Cumulative returns for each date\n",
        "    \"\"\"\n",
        "    total_returns = returns.apply(lambda row: np.sum(row), axis = 1) + 1\n",
        "    cumulative_returns = total_returns.cumprod()\n",
        "    cumulative_returns.iloc[0] = np.nan\n",
        "    return cumulative_returns\n",
        "\n",
        "project_tests.test_calculate_cumulative_returns(calculate_cumulative_returns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIozmVAMcCkN"
      },
      "outputs": [],
      "source": [
        "index_weighted_cumulative_returns = calculate_cumulative_returns(index_weighted_returns)\n",
        "optim_etf_returns = generate_weighted_returns(returns, optimal_single_rebalance_etf_weights)\n",
        "optim_etf_cumulative_returns = calculate_cumulative_returns(optim_etf_returns)\n",
        "\n",
        "project_helper.plot_benchmark_returns(index_weighted_cumulative_returns, optim_etf_cumulative_returns, 'Optimized ETF vs Index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(index_weighted_cumulative_returns)\n",
        "print(optim_etf_cumulative_returns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWuUAVtycCj_"
      },
      "source": [
        "**Tracking Error:**\n",
        "In order to check the performance of the smart beta portfolio, we can calculate the annualized tracking error against the index. Implement `tracking_error` to return the tracking error between the ETF and benchmark."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1vwU1c7cCj_",
        "outputId": "1b82ae70-de73-46cc-d454-357e7a27f470"
      },
      "outputs": [],
      "source": [
        "def tracking_error(benchmark_returns_by_date, etf_returns_by_date):\n",
        "    \"\"\"\n",
        "    Calculate the tracking error.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    benchmark_returns_by_date : Pandas Series\n",
        "        The benchmark returns for each date\n",
        "    etf_returns_by_date : Pandas Series\n",
        "        The ETF returns for each date\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    tracking_error : float\n",
        "        The tracking error\n",
        "    \"\"\"\n",
        "    assert benchmark_returns_by_date.index.equals(etf_returns_by_date.index)\n",
        "\n",
        "\n",
        "    return np.sqrt(365) * (benchmark_returns_by_date - etf_returns_by_date).std()\n",
        "\n",
        "project_tests.test_tracking_error(tracking_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xaK-eb7-cCj_"
      },
      "source": [
        "**View Data:**\n",
        "Let's generate the tracking error using `tracking_error`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKXZiu3wcCkA",
        "outputId": "17a69144-096b-4fd1-b930-c305a01c7339"
      },
      "outputs": [],
      "source": [
        "optim_etf_tracking_error = tracking_error(np.sum(index_weighted_returns, 1), np.sum(optim_etf_returns, 1))\n",
        "print('Optimized ETF Tracking Error: {}'.format(optim_etf_tracking_error))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sharp Ratio:**\n",
        "Now we will calculate sharpe ratio, a measure of risk-adjsuted returns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sharpe_ratio(returns_by_date, risk_free_rate_annual=0.03):\n",
        "    \"\"\"\n",
        "    Calculate the annualized Sharpe Ratio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns_by_date : Pandas Series\n",
        "        The asset returns for each date\n",
        "    risk_free_rate_annual : float, optional\n",
        "        The annual risk-free rate. Default is 0.03 (or 3%)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sharpe_ratio : float\n",
        "        The annualized Sharpe Ratio\n",
        "    \"\"\"\n",
        "    # Convert annual risk free rate to daily\n",
        "    risk_free_rate_daily = (1 + risk_free_rate_annual) ** (1/365) - 1\n",
        "    \n",
        "    # Calculate the average daily return and daily standard deviation\n",
        "    avg_daily_return = returns_by_date.mean()\n",
        "    daily_std_dev = returns_by_date.std()\n",
        "\n",
        "    # Calculate the annualized Sharpe Ratio\n",
        "    return np.sqrt(365) * (avg_daily_return - risk_free_rate_daily) / daily_std_dev\n",
        "\n",
        "project_tests.test_sharpe_ratio(sharpe_ratio)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_sharpe = sharpe_ratio(index_weighted_returns, risk_free_rate_annual=0.03)\n",
        "optim_etf_sharpe = sharpe_ratio(optim_etf_returns, risk_free_rate_annual=0.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**View Data:**\n",
        "Let's generate the sharp ratio using `sharpe_ratio`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(index_sharpe.iloc[0])\n",
        "print(optim_etf_sharpe.iloc[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Sortino Ratio:**\n",
        "The Sortino ratio is a variation of the Sharpe ratio, but instead of using the standard deviation of the returns, it uses the standard deviation of the negative returns (downside deviation). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def sortino_ratio(returns_by_date, risk_free_rate_annual):\n",
        "    \"\"\"\n",
        "    Calculate the Sortino ratio.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    returns_by_date : Pandas Series\n",
        "        The portfolio returns for each date\n",
        "    risk_free_rate_annual : float\n",
        "        The annual risk-free rate\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    sortino_ratio : float\n",
        "        The Sortino ratio\n",
        "    \"\"\"\n",
        "    # Calculate the annualized portfolio return\n",
        "    annualized_return = (1 + returns_by_date.mean()) ** 365 - 1\n",
        "    \n",
        "    # Calculate the annualized risk-free rate per period\n",
        "    risk_free_rate_per_period = (1 + risk_free_rate_annual) ** (1/365) - 1\n",
        "    \n",
        "    # Calculate the excess return\n",
        "    excess_return = annualized_return - risk_free_rate_annual\n",
        "    \n",
        "    # Calculate the downside deviation\n",
        "    negative_returns = returns_by_date[returns_by_date < risk_free_rate_per_period]\n",
        "    downside_deviation = negative_returns.std() * np.sqrt(365)\n",
        "    \n",
        "    # Calculate the Sortino ratio\n",
        "    sortino = excess_return / downside_deviation\n",
        "    \n",
        "    return sortino\n",
        "\n",
        "\n",
        "project_tests.test_sortino_ratio(sortino_ratio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "index_sortino = sortino_ratio(index_weighted_returns, risk_free_rate_annual=0.03)\n",
        "optim_etf_sortino = sortino_ratio(optim_etf_returns, risk_free_rate_annual=0.03)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**View Data:**\n",
        "Let's generate the sortino ratio using `sortino_ratio`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(index_sortino.iloc[0])\n",
        "print(optim_etf_sortino.iloc[0])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
